
import hashlib
import json
from time import time
from urllib.parse import quote_plus
from bson import ObjectId
import numpy as np
import pandas as pd
from pymongo import MongoClient
import os
import multiprocessing
from itertools import combinations, islice
from pymongo.errors import BulkWriteError, PyMongoError
from gen.scan import get_mongo_uri,make_key_corr
from bson import ObjectId

def calculate_trade_correlation_vectorized(df1, df2):
    if df1.empty or df2.empty:
        return 0.0, 0.0

    time_tolerance = pd.Timedelta(seconds=0)

    df1 = df1.sort_values('executionT')
    df2 = df2.sort_values('executionT')

    merged = pd.merge_asof(
        df1[['executionT', 'action']],
        df2[['executionT', 'action']],
        on='executionT',
        direction='nearest',
        tolerance=time_tolerance,
        suffixes=('_1', '_2')
    )

    # ƒêi·ªÅu ki·ªán vector h√≥a
    a1 = merged['action_1'].values
    a2 = merged['action_2'].values

    # N·∫øu kh√¥ng match s·∫Ω l√† NaN ‚Üí lo·∫°i b·ªè
    valid = ~np.isnan(a2)

    a1 = a1[valid]
    a2 = a2[valid]

    # Match signal logic
    matches = (
        (a1 == a2) |
        ((a1 == 1) & (a2 == 2)) |
        ((a1 == 2) & (a2 == 1)) |
        ((a1 == -1) & (a2 == -2)) |
        ((a1 == -2) & (a2 == -1))
    )

    matched_count = np.sum(matches)

    corr1 = round(matched_count / len(df1) * 100, 2)
    corr2 = round(matched_count / len(df2) * 100, 2)
    return max(corr1, corr2)

def calculate_combined_correlations(
    config_id,  # ƒê·ªïi t√™n 'id' th√†nh 'config_id' cho nh·∫•t qu√°n
    stras=None,
    max_workers=20,
    chunk_size=100000
):
    """
    T√≠nh to√°n t∆∞∆°ng quan alpha, s·ª≠ d·ª•ng ki·∫øn tr√∫c Producer-Consumer (Worker/Queue/Writer)
    gi·ªëng nh∆∞ h√†m t√≠nh t∆∞∆°ng quan stock chu·∫©n.
    """
    
    # === 0Ô∏è‚É£ Kh·ªüi t·∫°o k·∫øt n·ªëi DB ===
    # C√°c process con s·∫Ω k·∫ø th·ª´a k·∫øt n·ªëi n√†y, 
    # nh∆∞ng ch·ªâ writer_worker th·ª±c s·ª± d√πng n√≥ ƒë·ªÉ ghi.
    db = MongoClient(get_mongo_uri())['gen1_2']
    alpha_correl_coll = db["alpha_correl"]
    correlation_coll = db["correlation_results"]

    # === 1Ô∏è‚É£ Chu·∫©n b·ªã d·ªØ li·ªáu ===
    print("‚è≥ ƒêang chu·∫©n b·ªã d·ªØ li·ªáu (parsing trades)...")
    id_to_trade_df = {}

    def parse_trade_doc(doc):
        trades = doc.get("df_trade")
        if not isinstance(trades, list) or not trades:
            return None
        # Ki·ªÉm tra ƒë·ªãnh d·∫°ng c·ªßa alpha
        if not all(isinstance(t, dict) and "executionT" in t and "action" in t for t in trades):
            return None
        df = pd.DataFrame(trades)
        df["executionT"] = pd.to_datetime(df["executionT"], errors="coerce")
        df.dropna(subset=["executionT"], inplace=True)
        return df if not df.empty else None

    for doc in stras:
        _id = doc["_id"]
        trade_df = parse_trade_doc(doc)
        if trade_df is not None:
            id_to_trade_df[_id] = trade_df

    def chunked_iterable(iterable, size):
        it = iter(iterable)
        while True:
            chunk = list(islice(it, size))
            if not chunk:
                break
            yield chunk

    valid_ids = list(id_to_trade_df.keys())
    
    # === 2Ô∏è‚É£ L·ªçc c√°c c·∫∑p ƒë√£ t·ªìn t·∫°i (Gi·ªëng code chu·∫©n) ===
    print("üîé ƒêang l·∫•y danh s√°ch c·∫∑p ƒë√£ t·ªìn t·∫°i trong MongoDB...")
    str_ids = [str(i) for i in valid_ids]
    existing_pairs = set()
    
    # Query hi·ªáu qu·∫£ h∆°n thay v√¨ d√πng 2 $in l·ªõn
    for x in str_ids:
        cursor = correlation_coll.find(
            {"x": x, "y": {"$in": str_ids}},
            {"x": 1, "y": 1, "_id": 0}
        )
        for doc in cursor:
            existing_pairs.add(tuple(sorted((doc["x"], doc["y"]))))

    print(f"‚úÖ ƒê√£ c√≥ s·∫µn {len(existing_pairs):,} c·∫∑p trong DB ‚Äî s·∫Ω b·ªè qua.")

    # === 3Ô∏è‚É£ Sinh danh s√°ch c·∫∑p c·∫ßn x·ª≠ l√Ω (Gi·ªëng code chu·∫©n) ===
    all_pairs = []
    total_combinations = 0
    for id1, id2 in combinations(valid_ids, 2):
        total_combinations += 1
        key = tuple(sorted((str(id1), str(id2))))
        if key not in existing_pairs:
            all_pairs.append((id1, id2))

    print(f"üßÆ C√≤n l·∫°i {len(all_pairs):,} c·∫∑p c·∫ßn t√≠nh m·ªõi (tr√™n t·ªïng s·ªë {total_combinations:,} c·∫∑p).")
    
    chunks = list(chunked_iterable(all_pairs, chunk_size))
    total_chunks = len(chunks)
    total_pairs_to_process = len(all_pairs)

    print(f"üî¢ T·ªïng s·ªë c·∫∑p c·∫ßn x·ª≠ l√Ω: {total_pairs_to_process}")
    print(f"üì¶ T·ªïng s·ªë chunk (m·ªói chunk ~{chunk_size} c·∫∑p): {total_chunks}")

    if config_id:
        alpha_correl_coll.update_one(
            {"_id": ObjectId(config_id)},
            {"$set": {"process": {"done": len(existing_pairs), "total": total_combinations}}}
        )

    if not all_pairs:
        print("üèÅ Kh√¥ng c√≤n c·∫∑p n√†o ƒë·ªÉ x·ª≠ l√Ω. K·∫øt th√∫c.")
        alpha_correl_coll.update_one(
            {"_id": ObjectId(config_id)},
            {"$set": {"status": "done"}}
        )
        return

    # === 4Ô∏è‚É£ Writer process (Gi·ªëng code chu·∫©n) ===
    def writer_worker(q: multiprocessing.Queue, initial_done_count: int):
        total_written = initial_done_count
        MAX_BATCH_SIZE = 10000

        while True:
            results = q.get()
            if results == "STOP":
                print("üßæ Writer nh·∫≠n t√≠n hi·ªáu d·ª´ng, k·∫øt th√∫c.")
                break

            for i in range(0, len(results), MAX_BATCH_SIZE):
                sub_batch = results[i:i + MAX_BATCH_SIZE]
                try:
                    correlation_coll.insert_many(sub_batch, ordered=False)
                    total_written += len(sub_batch)
                    # C·∫≠p nh·∫≠t ti·∫øn ƒë·ªô
                    if config_id:
                        alpha_correl_coll.update_one(
                            {"_id": ObjectId(config_id)},
                            {"$set": {"process.done": total_written}}
                        )
                except BulkWriteError as bwe:
                    # B·ªè qua l·ªói tr√πng l·∫∑p (11000) nh∆∞ng v·∫´n ƒë·∫øm
                    valid_writes = len(sub_batch) - len(bwe.details.get("writeErrors", []))
                    total_written += valid_writes # Ch·ªâ ƒë·∫øm s·ªë l∆∞·ª£ng ghi th√†nh c√¥ng
                    if config_id:
                         alpha_correl_coll.update_one(
                            {"_id": ObjectId(config_id)},
                            {"$set": {"process.done": total_written}}
                        )
                    print(f"‚ö†Ô∏è Writer: B·ªè qua {len(bwe.details.get('writeErrors', []))} l·ªói (v√≠ d·ª•: tr√πng l·∫∑p).")
                    continue
                except Exception as e:
                    print(f"‚ùå Writer l·ªói MongoDB (batch nh·ªè): {e}")

        print(f"‚úÖ Writer ho√†n t·∫•t, t·ªïng ghi m·ªõi: {total_written - initial_done_count}")

    # === 5Ô∏è‚É£ Worker process: Ch·ªâ t√≠nh to√°n, g·ª≠i k·∫øt qu·∫£ ===
    def process_chunk(q, chunk):
        results = []
        for id1, id2 in chunk:
            x, y = str(id1), str(id2)
            
            # Kh√¥ng c·∫ßn ki·ªÉm tra existing_pairs ·ªü ƒë√¢y n·ªØa
            df1, df2 = id_to_trade_df[id1], id_to_trade_df[id2]
            
            # Logic t√≠nh to√°n c·ªßa alpha
            c = calculate_trade_correlation_vectorized(df1, df2)
            
            results.append({
                "x": x,
                "y": y,
                "c": round(c, 4), # Format c·ªßa alpha
            })

        if results:
            q.put(results) # G·ª≠i k·∫øt qu·∫£ v√†o queue

    # === 6Ô∏è‚É£ Kh·ªüi t·∫°o queue v√† writer ===
    q = multiprocessing.Queue(maxsize=max_workers * 4)
    writer = multiprocessing.Process(
        target=writer_worker, 
        args=(q, len(existing_pairs)) # Truy·ªÅn s·ªë l∆∞·ª£ng ƒë√£ ho√†n th√†nh ban ƒë·∫ßu
    )
    writer.start()

    # === 7Ô∏è‚É£ Kh·ªüi t·∫°o c√°c worker (Gi·ªëng code chu·∫©n) ===
    processes = []
    print(f"üöÄ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω {total_chunks} chunks v·ªõi t·ªëi ƒëa {max_workers} workers...")
    
    for chunk in chunks:
        while len(processes) >= max_workers:
            # Ch·ªù m·ªôt process con k·∫øt th√∫c tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu process m·ªõi
            for p in processes[:]:
                if not p.is_alive():
                    p.join()
                    processes.remove(p)
            import time
            time.sleep(0.05) # Ng·ªß 1 ch√∫t ƒë·ªÉ tr√°nh busy-waiting
            
        p = multiprocessing.Process(target=process_chunk, args=(q, chunk))
        p.start()
        processes.append(p)

    # ƒê·ª£i t·∫•t c·∫£ worker xong
    for p in processes:
        p.join()
    
    print("‚úÖ T·∫•t c·∫£ workers ƒë√£ ho√†n th√†nh t√≠nh to√°n.")

    # G·ª≠i t√≠n hi·ªáu d·ª´ng writer
    q.put("STOP")
    writer.join()

    # === 8Ô∏è‚É£ C·∫≠p nh·∫≠t process.done ch√≠nh x√°c (Gi·ªëng code chu·∫©n) ===
    if config_id:
        print("üîÑ ƒêang c·∫≠p nh·∫≠t l·∫°i s·ªë l∆∞·ª£ng ch√≠nh x√°c cu·ªëi c√πng...")
        seen = set()
        projection = {"x": 1, "y": 1, "_id": 0}
        for x in str_ids:
            cursor = correlation_coll.find(
                {"x": x, "y": {"$in": str_ids}},
                projection
            )
            for doc in cursor:
                seen.add(tuple(sorted((doc["x"], doc["y"]))))

        unique_pair_count = len(seen)
        alpha_correl_coll.update_one(
            {"_id": ObjectId(config_id)},
            {"$set": {"process.done": unique_pair_count, "status": "done"}}
        )
        print(f"‚úÖ ƒê√£ c·∫≠p nh·∫≠t l·∫°i ch√≠nh x√°c process.done = {unique_pair_count} v√† status = 'done'")
        
def run_corr(id):
    db =  MongoClient(get_mongo_uri())['gen1_2']
    alpha_correl_coll = db["alpha_correl"]
    alpha_correl = alpha_correl_coll.find_one({"_id": ObjectId(id)})
    alpha_name = alpha_correl.get("alpha_name", "")
    gen = alpha_correl.get("gen", "gen1_2")
    lst_configs = alpha_correl.get("configs", [])
    fee = 0.175
    start = alpha_correl.get("start", "2024_01_01")
    end = alpha_correl.get("end", "2025_01_01")
    list_ids = [make_key_corr(
            config=config,
            fee=fee,
            start=start,
            end=end,
            alpha_name=alpha_name,
            gen=gen
        ) for config in lst_configs]
    
    mongo_coll = db["stock"]
    exist_stra = list(mongo_coll.find({"_id": {"$in": list_ids}}))
    print(f"Found {len(exist_stra)} strategies in the database.")
    calculate_combined_correlations(
        config_id=id,
        stras=exist_stra,
        max_workers=20,
    ) 
    


